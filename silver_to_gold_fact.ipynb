{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd75e259-fd15-467f-8de1-6d8622be0224",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-05T05:08:32.5815245Z",
       "execution_start_time": "2025-04-05T05:08:32.5185004Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c51fc206-3e5c-4454-ae55-86b699ed3912",
       "queued_time": "2025-04-05T05:08:32.1086521Z",
       "session_id": "795606b0-112b-4455-bb44-86d8ff83b501",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 18,
       "statement_ids": [
        18
       ]
      },
      "text/plain": [
       "StatementMeta(, 795606b0-112b-4455-bb44-86d8ff83b501, 18, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run file_paths_updt Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05f2161e-352f-472d-96c6-de43ba09e89f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-05T05:08:34.389174Z",
       "execution_start_time": "2025-04-05T05:08:34.1029177Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "51bf4848-9f82-4eb8-b458-d5cda5ae3d07",
       "queued_time": "2025-04-05T05:08:34.1017116Z",
       "session_id": "795606b0-112b-4455-bb44-86d8ff83b501",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 19,
       "statement_ids": [
        19
       ]
      },
      "text/plain": [
       "StatementMeta(, 795606b0-112b-4455-bb44-86d8ff83b501, 19, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dcc158d-e7d1-4652-9964-19f2303b867a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-05T05:08:39.1582829Z",
       "execution_start_time": "2025-04-05T05:08:37.6782661Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "4c5e2ac9-718e-4e4a-ad31-d24d5c4edeb8",
       "queued_time": "2025-04-05T05:08:37.6770525Z",
       "session_id": "795606b0-112b-4455-bb44-86d8ff83b501",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 20,
       "statement_ids": [
        20
       ]
      },
      "text/plain": [
       "StatementMeta(, 795606b0-112b-4455-bb44-86d8ff83b501, 20, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_dim = spark.read.format('delta').load(Gold+'//'+'DimEmployee')\n",
    "loc_dim = spark.read.format('delta').load(Gold+'//'+'DimLocation')\n",
    "dept_dim = spark.read.format('delta').load(Gold+'//'+'DimDepartment')\n",
    "df = spark.read.format('delta').load(Silver+'//'+'hr_data_silver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a9b97-a4b9-4e73-8e3f-d823cd796288",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-05T05:12:50.2577387Z",
       "execution_start_time": "2025-04-05T05:12:49.9511928Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1148b15b-a265-43c7-8d9b-57f8bdda51d4",
       "queued_time": "2025-04-05T05:12:49.9498629Z",
       "session_id": "795606b0-112b-4455-bb44-86d8ff83b501",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 22,
       "statement_ids": [
        22
       ]
      },
      "text/plain": [
       "StatementMeta(, 795606b0-112b-4455-bb44-86d8ff83b501, 22, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run file_paths_updt Copy\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import sys\n",
    "\n",
    "emp_dim = spark.read.format('delta').load(Gold+'//'+'DimEmployee')\n",
    "loc_dim = spark.read.format('delta').load(Gold+'//'+'DimLocation')\n",
    "dept_dim = spark.read.format('delta').load(Gold+'//'+'DimDepartment')\n",
    "df = spark.read.format('delta').load(Silver+'//'+'hr_data_silver')\n",
    "\n",
    "def fact_Creation():\n",
    "    try:\n",
    "        df4 = df.join(loc_dim, on=['location_city', 'location_state'], how='inner')\n",
    "        df5 = df4.join(dept_dim, on='department', how='inner')\n",
    "\n",
    "# Transformations\n",
    "        df6 = df5.withColumn('total_employees', count('employee_id').over(Window.partitionBy('Department_id')))\\\n",
    "                 .withColumn('age', floor(months_between(current_date(), col(\"birthdate\")) / 12))\\\n",
    "                 .withColumn('avg_age', floor(avg('age').over(Window.partitionBy('Department_id'))))\\\n",
    "                 .withColumn(\"gender\", when(col(\"gender\") == \"M\", \"Male\")\n",
    "                              .when(col(\"gender\") == \"FM\", \"Female\")\n",
    "                              .otherwise('Other'))\n",
    "\n",
    "# Gender distribution\n",
    "        gender_counts = df6.groupBy(\"department\", \"gender\").count()\n",
    "\n",
    "        gender_percentages = gender_counts.withColumn(\n",
    "         \"percentage\",\n",
    "        round((col(\"count\") / sum(\"count\").over(Window.partitionBy(\"department\"))) * 100, 2)\n",
    "        ).withColumn(\n",
    "        \"gender_percentage\",\n",
    "         concat_ws(\"\", col(\"gender\"), lit(\":\"), col(\"percentage\").cast(\"string\"), lit(\"%\"))\n",
    "        )\n",
    "\n",
    "        df_7 = gender_percentages.groupBy(\"department\").agg(\n",
    "        concat_ws(\", \", collect_list(\"gender_percentage\")).alias(\"gender_distribution\")\n",
    "        )\n",
    "\n",
    "        df_8 = df6.join(df_7, on='department', how='inner')\n",
    "\n",
    "# Inactive employee calculation\n",
    "        df_with_in_active = df_8.withColumn(\n",
    "        'inactive_emp',\n",
    "        sum(when(col('termdate').isNotNull(), 1).otherwise(0)).over(Window.partitionBy('Department_id'))\n",
    "        )\n",
    "\n",
    "# Final fact table\n",
    "        window_spec = Window.orderBy('employee_id')\n",
    "\n",
    "        hr_fact = df_with_in_active.withColumn(\n",
    "        'turnover_rate', round((col('inactive_emp') / col('total_employees')) * 100, 2)\n",
    "        ).withColumn(\n",
    "        'fact_id', row_number().over(window_spec)\n",
    "        ).select(\n",
    "        'fact_id', 'emp_id_int', 'employee_id', 'location_id', 'department_id',\n",
    "        'total_employees', 'avg_age', 'gender_distribution', 'turnover_rate'\n",
    "        )\n",
    "\n",
    "# Save to Gold path\n",
    "        hr_fact.write.format('delta').mode('overwrite').save(Gold + '//' + 'Fact_Table')\n",
    "        print('count', hr_fact.count())     \n",
    "    except:\n",
    "        print(sys.exc_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb47dfca-871a-406c-be00-1ee4452ab670",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-05T02:20:05.1157753Z",
       "execution_start_time": "2025-04-05T02:19:22.953846Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "7cd7d84c-097f-476f-929b-fe94ef414e2f",
       "queued_time": "2025-04-05T02:19:22.9526406Z",
       "session_id": "ff612dc8-62ad-41b5-ac5a-a57e16f89141",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, ff612dc8-62ad-41b5-ac5a-a57e16f89141, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count 22214\n"
     ]
    }
   ],
   "source": [
    "fact_Creation()"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
